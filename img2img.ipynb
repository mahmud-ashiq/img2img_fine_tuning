{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f047dcb-1451-42d1-b513-29a9dfce1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from diffusers import DDPMScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf0a0e6-c301-4bd7-90b1-16d2ecf31746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Conda\\.conda\\envs\\tf\\lib\\site-packages\\transformers\\utils\\hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Initialize CLIP model and processor (just an example)\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caba59be-ad44-4217-9974-4d454c4b28a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7f600f45364c2abed5be887696cbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "pipeline = pipeline.to(\"cuda\")  # Move model to GPU\n",
    "pipeline.enable_model_cpu_offload()\n",
    "optimizer = torch.optim.AdamW(pipeline.unet.parameters(), lr=5e-6)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"scheduler\")\n",
    "\n",
    "# Transform pipeline\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1] for Stable Diffusion\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12487c24-be8f-4a47-a09c-65f6b7cea6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reeadData(src_dir, target_dir, limit):\n",
    "    src_list = []\n",
    "    target_list = []\n",
    "    i=0\n",
    "    src = sorted(os.listdir(src_dir), key=lambda x: int(x.split('.')[0]))\n",
    "    target = sorted(os.listdir(target_dir), key=lambda x: int(x.split('.')[0]))\n",
    "    images = zip(src, target)\n",
    "    for src, target in images:\n",
    "        try:\n",
    "            src_img, target_img = cv2.imread(os.path.join(src_dir, src)), cv2.imread(os.path.join(target_dir, target))   # read the image\n",
    "            src_img, target_img= cv2.cvtColor(src_img, cv2.COLOR_BGR2RGB), cv2.cvtColor(target_img, cv2.COLOR_BGR2RGB)\n",
    "            src_list.append(src_img)\n",
    "            target_list.append(target_img)\n",
    "            i +=1\n",
    "            if i == limit:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return src_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80ebe48-5298-4ff0-9cce-adb52a0dd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src_images, target_images, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_images (list): List of source images.\n",
    "            target_images (list): List of target images.\n",
    "            transform (callable, optional): Transform to be applied to the images.\n",
    "            image_limit (int, optional): Maximum number of images to include in the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        self.src_images = src_images\n",
    "        self.target_images = target_images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.transform(self.src_images[idx]) if self.transform else self.src_images[idx]\n",
    "        target = self.transform(self.target_images[idx]) if self.transform else self.target_images[idx]\n",
    "        return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c183da4-3b34-473c-9dd3-5c607d9b14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_images, target_images = reeadData(\"./dataset/face/\", \"./dataset/comics/\", limit=10)\n",
    "dataset = PairedDataset(src_images, target_images, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbda443f-9a57-49a7-8ec3-fdf46bb049a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CLIPProjector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(512, 768)  # Project from 512 to 768 dimensions\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize the projector\n",
    "clip_projector = CLIPProjector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06025029-b03f-4d04-8dda-ffa5706ed732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_hidden_states(src):\n",
    "    \"\"\"\n",
    "    Processes the input `src` (image tensor) through the CLIP model to get the encoder hidden states.\n",
    "\n",
    "    Args:\n",
    "        src (torch.Tensor): A tensor of shape [batch_size, 3, height, width].\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Processed encoder hidden states of shape [batch_size, sequence_length, hidden_size].\n",
    "    \"\"\"\n",
    "    # Preprocess the image tensor using the CLIP processor\n",
    "    inputs = clip_processor(images=src, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Get image features from the CLIP model\n",
    "    output = clip_model.get_image_features(**inputs)\n",
    "\n",
    "    # Print the shape of the output (should be [batch_size, 512])\n",
    "    print(\"Output shape from CLIP:\", output.shape)\n",
    "\n",
    "    # Project the output from 512 to 768 channels\n",
    "    projected_output = clip_projector(output)\n",
    "\n",
    "    # Print the new shape (should be [batch_size, 768])\n",
    "    print(\"Projected output shape:\", projected_output.shape)\n",
    "\n",
    "    # Reshape to match the required format [batch_size, sequence_length, 768]\n",
    "    # Here sequence_length is 1 (one token per image)\n",
    "    batch_size = projected_output.shape[0]\n",
    "    sequence_length = 1  # Sequence length is 1 for image-based input\n",
    "    hidden_size = 768  # We projected to 768 channels\n",
    "\n",
    "    # Reshaping output to [batch_size, sequence_length, hidden_size]\n",
    "    hidden_states = projected_output.view(batch_size, sequence_length, hidden_size)\n",
    "\n",
    "    # Now the output is 3D with shape [batch_size, 1, 768]\n",
    "    print(\"Final hidden states shape:\", hidden_states.shape)\n",
    "\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990190ea-8516-4e64-88e8-049e45515c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                                  | 0/10 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 512])\n",
      "Output shape from CLIP: torch.Size([1, 512])\n",
      "Projected output shape: torch.Size([1, 768])\n",
      "Final hidden states shape: torch.Size([1, 1, 768])\n",
      "Output shape from CLIP: torch.Size([1, 512])\n",
      "Projected output shape: torch.Size([1, 768])\n",
      "Final hidden states shape: torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # Number of epochs\n",
    "    pipeline.unet.train()\n",
    "    for src, target in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        src, target = src.to(\"cuda\"), target.to(\"cuda\")\n",
    "\n",
    "        # Add noise to the target image\n",
    "        noise = torch.randn_like(target).to(\"cuda\")\n",
    "        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (target.size(0),), device=\"cuda\")\n",
    "        noisy_target = noise_scheduler.add_noise(target, noise, timesteps)\n",
    "        print(noisy_target.shape)\n",
    "\n",
    "        # Add an extra channel to noisy_target to match UNet input requirements\n",
    "        noisy_target = torch.cat([noisy_target, torch.randn_like(noisy_target[:, :1, :, :])], dim=1)  # Add a noise channel\n",
    "        encoder_hidden_states(src)\n",
    "        # Forward pass\n",
    "        unet_output = pipeline.unet(noisy_target, timestep=timesteps, encoder_hidden_states=encoder_hidden_states(src))\n",
    "        print(unet_output.sample.shape)\n",
    "        noise_pred = unet_output.sample\n",
    "        \n",
    "\n",
    "        # Compute loss (mean squared error)\n",
    "        loss = torch.nn.functional.mse_loss(noise_pred, noise)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} completed with loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edc7397-6a25-4850-94d6-bb452e644de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bbc679-ea13-45a8-b403-91b03bbd9c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
